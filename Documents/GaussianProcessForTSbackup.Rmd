---
title: "Gaussian Processes for Time Series"
output: pdf_document
bibliography: bibliography.bib
---

# Bayesian Time Series Analysis

Given some time series data $D = \{(x_i,y_i)\}$ where $x_i$ is say a time point and $y_i$ is some reading at this time, we consider it as a regression problem:

$$y_i(\boldsymbol x) = f_i(\boldsymbol x) + \eta,$$

where $f$ is unknown and and $\eta$ is a random additive noise process, with the goal in mind to evaluate a form of $f$ and infer a probability distribution for $y$ given an input $x$. Throughout we will use the notation $\boldsymbol f ( \boldsymbol x) = [ f_1(\boldsymbol x), f_2(\boldsymbol x), \dots,  f_n(\boldsymbol x) ]$.

# Gaussian Processes

To condudct this analysis we consider a Gaussian Process (a good intro to which lies in [@rasmussen2003gaussian]), which is completely defined by the mean function and covariance function:

$$ m(\boldsymbol{x}) = \mathbb{E}(\boldsymbol f(\boldsymbol{x})) $$
$$ k(\boldsymbol{x},\boldsymbol{x'}) = \mathbb{E}[\{\boldsymbol f(\boldsymbol{x})-m(\boldsymbol{x})\}\{\boldsymbol f(\boldsymbol{x'})-m(\boldsymbol{x'})\}] $$

and we write the process as: $f(\boldsymbol{x}) \sim \mathcal{GP}(m(\boldsymbol{x}),k(\boldsymbol{x},\boldsymbol{x'}))$, and for simplicity often set the mean function to zero.

## Assumptions

A Gaussian process is defined as a collection of random variables where $\boldsymbol{x}$ is the input of time series indices, and in our case $\boldsymbol f(\boldsymbol{x})$ represents a time series. Hence we are assuming that we fullfil a consistency requriement, that is:

$$(y_1,y_2) \sim \mathcal N(\boldsymbol{\mu,\Sigma}) \implies y_1 \sim \mathcal N(\boldsymbol{\mu_1,\Sigma}_1), $$

for the relevant submatrix $\boldsymbol{\Sigma_1}$. That is, the examination of a large set of variables does not effect the distribution of each of its subsets, a criterion automatically fulfilled when we specify a kernel function for the covariance matrix. 

## The model

A Gaussian process can be obtained from a Bayesian liner regression model $f(\boldsymbol{x})= \phi(\boldsymbol{x})^\top \boldsymbol{w}$ where $\boldsymbol{w} \sim \mathcal N(\boldsymbol{0,\Sigma_p})$ - then the means and kernel functions become:

$$m(\boldsymbol x) =\mathbb{E}[\boldsymbol f(\boldsymbol{x})] = \phi(\boldsymbol{x})^\top\mathbb E [\boldsymbol w] = 0 $$
and

$$k(\boldsymbol{x,x'}) =  \mathbb{E}[\boldsymbol f(\boldsymbol{x}) \boldsymbol f(\boldsymbol{x'})] =  \phi(\boldsymbol{x})^\top \Sigma_p  \phi(\boldsymbol{x}')$$
For our time series data we will use the most widely used kernel function used for this class of problem [@roberts2013gaussian], the squared exponential function:

$$ k(x_i,x_j) = \sigma^2 \exp\bigg[-\frac{1}{2l}(x_i-x_j)^2 \bigg] $$
With hyperparameters $\sigma$ and $l$, choices of which can result in very different curves,  $\sigma$ effectively controls the gain of the function, and $l$ can be thought of as a smoothing parameter. It is known, [@rasmussen2003gaussian], that this corresponds to a regression with infinite basis functions.

# Fitting

## Prediction using Noisy Observations

It makes sense, for time series data, to assume input time indeces $x_i = i, i = 1, \dots$ are noiseless, and that the randomness comes with the values at each timestamp $y_i$. Let $X$ be the input set for our known values, and $X_*$ be those of the values we wish to predict with a function $\hat {\boldsymbol f}$ .So given a dataset we have access to noisy values $y = f(\boldsymbol x) + \varepsilon$ where $\varepsilon \sim \mathcal N(0,\sigma_n^2)$, hence the prior induced is:

$$ cov(\boldsymbol y) = K(X,X)+ \sigma^2_n\boldsymbol I,$$
and we can now, using consistency, write down the joint distribution of observed values $\boldsymbol y$ and function values $\boldsymbol f_*$:


$$\begin{bmatrix}
 \boldsymbol y \\
 \boldsymbol f_*
\end{bmatrix}
= \mathcal N\bigg(\boldsymbol 0 , 
\begin{bmatrix}
  K(X,X)+ \sigma^2_n\boldsymbol I &  K(X,X_*) \\
 K(X_*,X) & K(X_*X_*)
\end{bmatrix}
\bigg)$$

Using results on conditional Gaussian distributions [@bishop2006pattern], we can write down predictive equations:

$$  \boldsymbol f_* \mid X,  \boldsymbol y, X_* \sim \mathcal N \bigg( \hat{\boldsymbol f_*}, cov(\boldsymbol{f_*}) \bigg),$$

where

$$\hat{\boldsymbol f_*} = K(X_*,X)[K(X,X) + \sigma^2_n\boldsymbol I]^{-1} \boldsymbol y,$$
$$cov(\boldsymbol{f_*}) = K(X_*,X_*) - K(X_*,X)[ K(X,X) + \sigma^2_n\boldsymbol I]^{-1}K(X,X_*).$$

The next section will be dedicated to exploring this scheme using $\hat{\boldsymbol f_*}$, the conditional mean, as our predictive function on some real data - the `Fit_GP` function in our package gives a method of how to fit a Gaussian Process to time series data.


# Real Data

Casting our minds back to our factor analysis, we now present fits of a Gaussian process to a reduction of the undifferenced SP500 multivariate time series to a single variate factor of $251$ values through time, fitted using the `myPCA` function in the `portopt` package. Fitting a Gaussian Process to the factor series corresponding to the greatest varience of the original data will allow us to analyse  underlying behaviour of the SP500 data as a whole. We will now see how Gaussian processes perform when trying to interpolate and predict missing data.


## Choice of Kernel parameter $l$ for Predicting Missing Data

The problem set up here is very simple, suppose we are in the setting that a section of our factor series data has gone missing, either through collection, computer, or human error and we wish to try and learn the behaviour of our time series in order to interpolate to fill the gap in the readings. Recall our kernel function:


$$ k(x_i,x_j) = \sigma^2 \exp\bigg[-\frac{1}{2l}(x_i-x_j)^2 \bigg] $$

To simplify the problem let's fix our the gain parameter to $\sigma = 200$, and let's learn $l$ to perform an interpolation for a missing set of 25 consecutive readings. Let's look at how our model prediction function, fit using the `Fit_GP` function in the `portop` package , looks for various choices of $l$. Here the Gaussian Process has been fit on the factor series readings with a section of 25 readings missing from $175, \dots, 200$ for which our conditional mean serves as our prediction function, we plot it against the values the process is fitted on in blues, and the missing values in red:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(readr)
# run fit_factor_model.R
setwd("~/Documents/FinProject/portopt")
data <- read_csv('data/combined_close_data.csv', col_types = cols())
data <- data[-c(1,2)] %>% drop_na() %>% matrix(., nrow = 239,ncol = 419)

oil_close_data <- read.csv("data/oil_close_2018.csv") #import data
close_data_2018 <- subset(oil_close_data, select = -c(X.1, DATE, DCOILWTICO, ADT)) 
log_close <- close_data_2018 #take log
diff_log <- ts(log_close) #diff series


Covar <- cov(diff_log) #find covariance
PCA <- princomp(Covar) #take PCA of covariance
PCA <- prcomp(diff_log) #find PCs of sample covariance

#' Squared Exponential Kernel Function
#'
#' @param x Kernel input
#' @param y Kernel input
#' @param sig Hyper parameter
#' @param l Hyper parameter
#'
#' @return
#' @export
#'
#' @examples
squared_exp_kernel <- function(x,y,sig,l){
  return(sig^2 * exp(-1/2/l^2 * (x-y)^2))
}

#' Produce Covariance matrix
#'
#' @param x Kernel input
#' @param y Kernel input
#' @param sig Hyper parameter
#' @param l Hyper parameter
#'
#' @return
#' @export
#'
#' @examples
Kernel <- function(x,y,sig,l){
  outer(x, y, Vectorize(function(x,y)  squared_exp_kernel(x,y,sig,l)))
}


#' Fit Gaussian Process with squared exp kernel
#'
#' @param x Input time indicies
#' @param y Known values for input indices
#' @param sig Kernel hyper parameter
#' @param sigma_n Noise parameter
#' @param l Kernel hyper parameter
#' @param xstar Time indices to predict values for
#'
#' @return
#' @export
#'
#' @examples
Fit_GP <- function(x,y,sig,sigma_n,l,xstar){
  K <- Kernel(x,x, sig,l) # produce kernel matrix
  L <- chol(K + sigma_n^2*diag(length(x)))

  alpha <- chol2inv(L) %*% y

  kstar <- Kernel(x,xstar,sig,l)

  fhatstar <- t(kstar)%*%alpha    # Predictive mean


  v <- solve(t(L),kstar)
  
  Varhatstar <- Kernel(xstar,xstar,sig,l) - t(v)%*%v
  
  logmarglikelihood <-  -0.5*t(y)%*%alpha - sum(diag(L)) - (length(x)/2)*log(2*pi)  #log likelihood
  val <- list(mean = fhatstar, variance = Varhatstar, loglikelihood = logmarglikelihood)
  return(val)
}


Y <- matrix(diff_log, nrow = dim(diff_log)[1], ncol = dim(diff_log)[2] )
loadings <- PCA$rotation[,1] #extract first 10 components
loadings <- as.matrix(loadings)
factor_series <- (Y%*%loadings) %*% solve(t(loadings)%*%loadings) #compute time series of factors, unnormalised




y <- as.vector(factor_series)
x <- 1:length(y) 

```



```{r,echo=FALSE, message=FALSE, warning=FALSE}

missing_indices <- c(175:200)

x_all <- 1:251
yhat <- y[-missing_indices]
x <- x_all[-missing_indices]
xstar <- x_all

predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l = 15,xstar=xstar)      ## bigger the gap bigger the bandwith

dftrue <- as.data.frame(cbind(missing_indices,y[missing_indices]))

dfinput <- as.data.frame(cbind(x,yhat))

dfpredict <-  as.data.frame(cbind(xstar,predict$mean))

pl1 <- ggplot(data = dfinput, aes(x,yhat))+
      geom_point(color = "#00AFBB", size = 1) +
     labs(x = 'time', y = 'value') +
      geom_point(data = dftrue, aes(x = missing_indices, y = V2), color = '#FC4E07')+
       geom_line(data = dfpredict, aes(x = xstar, y = V2) ,color = "#E7B800", size = 1)+ 
      ggtitle('l = 15')
```


```{r,echo=FALSE, message=FALSE, warning=FALSE}
missing_indices <- c(175:200)

x_all <- 1:251
yhat <- y[-missing_indices]
x <- x_all[-missing_indices]
xstar <- x_all

predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l = 20,xstar=xstar)      ## bigger the gap bigger the bandwith

dftrue <- as.data.frame(cbind(missing_indices,y[missing_indices]))

dfinput <- as.data.frame(cbind(x,yhat))

dfpredict <-  as.data.frame(cbind(xstar,predict$mean))

pl2 <- ggplot(data = dfinput, aes(x,yhat))+
      geom_point(color = "#00AFBB", size = 1) +
     labs(x = 'time', y = 'value') +
      geom_point(data = dftrue, aes(x = missing_indices, y = V2), color = '#FC4E07')+
       geom_line(data = dfpredict, aes(x = xstar, y = V2) ,color = "#E7B800", size = 1)+
  ggtitle('l = 20') 

```


```{r,echo=FALSE, message=FALSE, warning=FALSE}
missing_indices <- c(175:200)

x_all <- 1:251
yhat <- y[-missing_indices]
x <- x_all[-missing_indices]
xstar <- x_all

predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l = 70,xstar=xstar)      ## bigger the gap bigger the bandwith

dftrue <- as.data.frame(cbind(missing_indices,y[missing_indices]))

dfinput <- as.data.frame(cbind(x,yhat))

dfpredict <-  as.data.frame(cbind(xstar,predict$mean))

pl3 <- ggplot(data = dfinput, aes(x,yhat))+
      geom_point(color = "#00AFBB", size = 1) +
     labs(x = 'time', y = 'value') +
      geom_point(data = dftrue, aes(x = missing_indices, y = V2), color = '#FC4E07')+
       geom_line(data = dfpredict, aes(x = xstar, y = V2) ,color = "#E7B800", size = 1)+
  ggtitle('l = 20')
```





```{r,echo=FALSE, message=FALSE, warning=FALSE}
missing_indices <- c(175:200)

x_all <- 1:251
yhat <- y[-missing_indices]
x <- x_all[-missing_indices]
xstar <- x_all

predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l = 70,xstar=xstar)      ## bigger the gap bigger the bandwith

dftrue <- as.data.frame(cbind(missing_indices,y[missing_indices]))

dfinput <- as.data.frame(cbind(x,yhat))

dfpredict <-  as.data.frame(cbind(xstar,predict$mean))

pl3 <- ggplot(data = dfinput, aes(x,yhat))+
      geom_point(color = "#00AFBB", size = 1) +
     labs(x = 'time', y = 'value') +
      geom_point(data = dftrue, aes(x = missing_indices, y = V2), color = '#FC4E07')+
       geom_line(data = dfpredict, aes(x = xstar, y = V2) ,color = "#E7B800", size = 1)+
      ggtitle('l = 70')

```

```{r,echo=FALSE, message=FALSE, warning=FALSE}
missing_indices <- c(175:200)

x_all <- 1:251
yhat <- y[-missing_indices]
x <- x_all[-missing_indices]
xstar <- x_all

predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l = 100,xstar=xstar)      ## bigger the gap bigger the bandwith

dftrue <- as.data.frame(cbind(missing_indices,y[missing_indices]))

dfinput <- as.data.frame(cbind(x,yhat))

dfpredict <-  as.data.frame(cbind(xstar,predict$mean))

pl4 <- ggplot(data = dfinput, aes(x,yhat))+
      geom_point(color = "#00AFBB", size = 1) +
     labs(x = 'time', y = 'value') +
      geom_point(data = dftrue, aes(x = missing_indices, y = V2), color = '#FC4E07')+
       geom_line(data = dfpredict, aes(x = xstar, y = V2) ,color = "#E7B800", size = 1)+
  ggtitle('l = 100')

```


```{r,echo=FALSE, message=FALSE, warning=FALSE}
library(gridExtra)
grid.arrange(pl1,pl2,pl3,pl4, nrow=2)
```

We see that increasing $l$ decreases the flexibility of the prediction function. An interpretation of $l$ given in [@rasmussen2003gaussian] is that it can be thought of as a 'decay time' relative to $\sigma$, that is, how quickly should expect the process to change between readings - this interpretation is backed up by these observations as for lower values of $l$ we see the prediction function moving further from the known values.

   Let's define a loss function for our choice of l, it is defined only over the missing data indexed $j = 1, \dots, 25$, we take the squared loss of the predictions:
   
$$ L(l)= \sum_{j=1}^{25}(y_{true}^{(j)} - f_j(\boldsymbol x) )^2, $$
where $f_i(\boldsymbol x)$ is our predictive mean at timestamp $i$. Let's plot this over reasonable values of $l$ - preliminary analysis shows a loss which grows for extreme values of $l$ so plotting over $l \in [30,70]$ will include the relevant optimal points: 







```{r,echo=FALSE, message=FALSE, warning=FALSE}
missing_indices <- c(175:200)
squared_loss <- function(l,missing_indices,y){
  x_all <- 1:251
  yhat <- y[-missing_indices]
  x <- x_all[-missing_indices]
  xstar <- x_all
  predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l,xstar=xstar)
  preds <- predict$mean[missing_indices]
  
  return(sum( (y[missing_indices]- preds)^2))
}

squared_loss(5,c(175:200),y)


# opt <- optim(40, function(l) squared_loss(l,c(175:200),y), method = 'Brent', lower = 30, upper = 60) ==  49.68671  where it atains 2437969344
```





```{r,echo=FALSE, message=FALSE, warning=FALSE}
l <-  seq(from=32, to = 65, by = 1)
t <- sapply(l, function(l) squared_loss(l,c(175:200),y))

df1 <- as.data.frame(cbind(l,t))

pl5 <- ggplot(data = df1, aes(x=l,y=t)) + geom_line(color = "#00AFBB") + labs(x = 'l', y = 'loss', title = 'Loss as a function of l')
pl5
```

We see that our loss has a multiple local optima, a phenomenon frequently observed with optimisation of kernel paramenters [@rasmussen2003gaussian], it appears we have a global minimum. Running an optimisation with the 'Brent' method indeed tells us that our squared loss is minimised for $l=49.68671$ attaining a value of 2437969344:

```{r,echo=FALSE, message=FALSE, warning=FALSE}
missing_indices <- c(175:200)

x_all <- 1:251
yhat <- y[-missing_indices]
x <- x_all[-missing_indices]
xstar <- x_all

predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l = 49.68671,xstar=xstar)      ## bigger the gap bigger the bandwith

dftrue <- as.data.frame(cbind(missing_indices,y[missing_indices]))

dfinput <- as.data.frame(cbind(x,yhat))

dfpredict <-  as.data.frame(cbind(xstar,predict$mean))

pl6 <- ggplot(data = dfinput, aes(x,yhat))+
      geom_point(color = "#00AFBB", size = 1) +
     labs(x = 'time', y = 'value') +
      geom_point(data = dftrue, aes(x = missing_indices, y = V2), color = '#FC4E07')+
       geom_line(data = dfpredict, aes(x = xstar, y = V2) ,color = "#E7B800", size = 1)+
  ggtitle('l = 49.68671')

pl6
```


# Testing

So we have learned a value of $l$ over the training set $i = 175, \dots, 200$, let's see if our learned kernel generalises to fit unseen gaps:



```{r,echo=FALSE, message=FALSE, warning=FALSE}
missing_indices <- c(25:50)

x_all <- 1:251
yhat <- y[-missing_indices]
x <- x_all[-missing_indices]
xstar <- x_all

predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l = 49.68671,xstar=xstar)      ## bigger the gap bigger the bandwith

dftrue <- as.data.frame(cbind(missing_indices,y[missing_indices]))

dfinput <- as.data.frame(cbind(x,yhat))

dfpredict <-  as.data.frame(cbind(xstar,predict$mean))

pl11 <- ggplot(data = dfinput, aes(x,yhat))+
      geom_point(color = "#00AFBB", size = 1) +
     labs(x = 'time', y = 'value') +
      geom_point(data = dftrue, aes(x = missing_indices, y = V2), color = '#FC4E07')+
       geom_line(data = dfpredict, aes(x = xstar, y = V2) ,color = "#E7B800", size = 1)+
  ggtitle('missing data i = 25,...,50')

```

```{r,echo=FALSE, message=FALSE, warning=FALSE}
missing_indices <- c(50:75)

x_all <- 1:251
yhat <- y[-missing_indices]
x <- x_all[-missing_indices]
xstar <- x_all

predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l = 49.68671,xstar=xstar)      ## bigger the gap bigger the bandwith

dftrue <- as.data.frame(cbind(missing_indices,y[missing_indices]))

dfinput <- as.data.frame(cbind(x,yhat))

dfpredict <-  as.data.frame(cbind(xstar,predict$mean))

pl12 <- ggplot(data = dfinput, aes(x,yhat))+
      geom_point(color = "#00AFBB", size = 1) +
     labs(x = 'time', y = 'value') +
      geom_point(data = dftrue, aes(x = missing_indices, y = V2), color = '#FC4E07')+
       geom_line(data = dfpredict, aes(x = xstar, y = V2) ,color = "#E7B800", size = 1)+
  ggtitle('missing data i = 50,...,75')

```

```{r,echo=FALSE, message=FALSE, warning=FALSE}
missing_indices <- c(100:125)

x_all <- 1:251
yhat <- y[-missing_indices]
x <- x_all[-missing_indices]
xstar <- x_all

predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l = 49.68671,xstar=xstar)      ## bigger the gap bigger the bandwith

dftrue <- as.data.frame(cbind(missing_indices,y[missing_indices]))

dfinput <- as.data.frame(cbind(x,yhat))

dfpredict <-  as.data.frame(cbind(xstar,predict$mean))

pl13 <- ggplot(data = dfinput, aes(x,yhat))+
      geom_point(color = "#00AFBB", size = 1) +
     labs(x = 'time', y = 'value') +
      geom_point(data = dftrue, aes(x = missing_indices, y = V2), color = '#FC4E07')+
       geom_line(data = dfpredict, aes(x = xstar, y = V2) ,color = "#E7B800", size = 1)+
  ggtitle('missing data i = 100,...,125')

```

```{r,echo=FALSE, message=FALSE, warning=FALSE}
missing_indices <- c(200:225)

x_all <- 1:251
yhat <- y[-missing_indices]
x <- x_all[-missing_indices]
xstar <- x_all

predict <- Fit_GP(x,yhat,sig=200,sigma_n=0.1,l = 49.68671,xstar=xstar)      ## bigger the gap bigger the bandwith

dftrue <- as.data.frame(cbind(missing_indices,y[missing_indices]))

dfinput <- as.data.frame(cbind(x,yhat))

dfpredict <-  as.data.frame(cbind(xstar,predict$mean))

pl14 <- ggplot(data = dfinput, aes(x,yhat))+
      geom_point(color = "#00AFBB", size = 1) +
     labs(x = 'time', y = 'value') +
      geom_point(data = dftrue, aes(x = missing_indices, y = V2), color = '#FC4E07')+
       geom_line(data = dfpredict, aes(x = xstar, y = V2) ,color = "#E7B800", size = 1)+
  ggtitle('missing data i = 200,...,225')

```

```{r,echo=FALSE, message=FALSE, warning=FALSE}
library(gridExtra)
grid.arrange(pl11,pl12,pl13,pl14, nrow=2)
```
Here we see our value for $l$ tested on various sets of missing data with loss values $1778024179$, $2917764245$, $194541423$, and $6415421161$, respectively - Suggesting that the method of learning $l$ does not generalise very well.

## Conclusions 

We have seen how taking a Bayesian view of modelling a time series allows us to model it as a Gaussian Process equivalent to using infinite basis functions in a regression setting, modelling as such leads to a rich arsenal for inference, notably a predictive conditional mean, variance and likelihood obtained using results from the rules of conditional Gaussian distributions. Whilst the attempt at a method to model and predict missing data values has proved to generalise weakly, an insight was gained as to how the choice of $l$ for the squared exponential kernel effects the fit of the Gaussian Process. Further methods to explore for such problems could include utilising the predictive variance and conditional likelihood into the optimisation of of the parameters of the kernel function such as those suggested in [@roberts2013gaussian].


```{r,echo=FALSE, message=FALSE, warning=FALSE,eval= FALSE}
missing_indices <- c(25:50)    ## 194541423
squared_loss(5,missing_indices,y)

missing_indices <- c(50:75)  ## 146936251
squared_loss(5,missing_indices,y)

missing_indices <- c(100:125)  ## 6415421161
squared_loss(5,missing_indices,y)

missing_indices <- c(200:225)
squared_loss(5,missing_indices,y) ## 995675551
```


# References











