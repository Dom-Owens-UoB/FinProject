---
title: 'Finance Project: Forecasting the S&P 500'
author: "Dom Owens"
date: "18/11/2019"
output:
  html_document: default
  pdf_document: default
---



## Factor Analysis

Within this project, we will use **Factor Analysis** as a dimension reduction technique for our mutlivariate time series $\mathbf{x}_t$. The problem is treated as follows:

We wish to reduce the dimension of an observable random vector $\mathbf{x} \in \mathbb{R}^p$ to a smaller vector $\mathbf{x} \in \mathbb{R}^m$ of latent variabes, where $m << p$.

We do this by expressing each $x_i$ as a linear combination of the factors: 
$$x_i = \lambda_{i1} f_1 + ... + \lambda_{im} f_m + \epsilon_i \hspace{30pt} \mathbf{x} = \Lambda \mathbf{f} + \boldsymbol{\epsilon}$$ 

Here, $\Lambda$ are the **factor loadings**, and $\boldsymbol{\epsilon}$ are errors.

We make the following assumptions:

- $E(\boldsymbol{\epsilon}) = \boldsymbol{0}$, $E(\boldsymbol{f}) = \boldsymbol{0}$, $E(\boldsymbol{x}) = \boldsymbol{0}$ (WLOG)

- $E(\boldsymbol{\epsilon \epsilon^T}) = \boldsymbol{\Psi}$ is a diagonal matrix

- $E(\boldsymbol{f f^T}) = \boldsymbol{I}_m$, so that the factors are independent

- For inferential purposes, we make distributional assumptions on $\boldsymbol{f}$ or $\boldsymbol{x}$ (often multivariate normality)


We will be working with **time series** data and models, meaning our observations $\mathbf{x}_t$ are indexed in time by $t \in \{ 0, 1, ... T \}$.
We further assume that

- The **Covariance** matrix $\Sigma$ is constant with respect to $t$ (this is the **static** model, as opposed to the more complicated **dynamic** model)

- (further assumptions - second order stn.ry, differences etc.)

Indeed, underlying the whole idea of forecasting financial markets is the **Big Assumption**, which is that economic activity in the near future will closely resemble economic activity in the past.

### Estimation

As opposed to the similar-looking regression problem $$\boldsymbol{y} = X \boldsymbol{\beta} + \boldsymbol{\epsilon} $$, in which the desgin matrix $X$ is known, we know neither $\boldsymbol{f}$ nor $\Lambda$; hence, any "best-fit" solutions will not be unique.

For conducting estimation in practice, we often find $\hat{\Lambda}$ and $\hat{\Phi}$, then find $\boldsymbol{\hat{f}}$.
In disciplines such as finance and economics, the factors can be pre-specified according to theoretical justifications (see the [Fama-French factor model]{https://www.investopedia.com/terms/f/famaandfrenchthreefactormodel.asp} ); we will use a mathematical approach instead.

One way of finding estimates leverages Principal Components Analysis (PCA), called **Principal Factor Analysis (PFA)**. is what we will use in this project.

We work with the covariance and sample covariances matrices $$ \Sigma = \Lambda \Lambda^T + \Psi \text{ and } 
S = \frac{1}{T}XX^T$$
Suppose that we have the principal components decomposition 
$$ \mathbf{x}_t = A^T \mathbf{z}_t  $$
where $A \in \mathbb{R}^{p \times n}$ is a matrix consisting of eigenvectors $\boldsymbol{\alpha_i}$, each corresponding to an eigenvalue $l_i$ in decreasing order. $\mathbf{z} \in \mathbb{R}^p $ and $p$ is the number of different series being measured.  

By partitioning the decomposition into the principal $m$ and minor $p-m$ components, we obtain a factor analysis and error accordingly:
$$\mathbf{x}_t = (A_m|A^*_{p-m} )^T  \left( \frac{\mathbf{z}_{m,t}} {\mathbf{z}^*_{p-m,t}} \right) $$
$$ = A_m^T \mathbf{z}_{m,t} + (A^*_{p-m})^T \mathbf{z}^*_{p-m,t} $$
$$= \Lambda \mathbf{f}_t + \boldsymbol{\epsilon}_t $$

Substituting in the sample covariance and normalising gives us our estimates (for a model with $\Psi = \sigma^2 I$ this maximises the log-likelihood function (PRML p.548) *(are we assuming this?)*)

$$  \hat{\Lambda} = \sqrt{p}A^T_m , \hat{\Phi} ={p}A^{*T}_{p-m}A^{*}_{p-m} $$



### Factor Analysis: Selecting the number of factors $m$
We can select the number of factors to use, $m$, using the knowledge that each factor explains a decreasing amount of the total variance, given our static model estimated with PCA.

We might plot the Scree plot (with `screeplot`) and identify a drop-off in the variance explained, or pick an amount of factors sufficient to explain, say, 70% of the variance. Alternatively, information criteria can give a systematic means of selecting $m$, though these are slightly more complicated.

### References
Pattern Recognition and Machine Learning, Christopher Bishop

[Dyanmic Factor Models]{http://www.barigozzi.eu/MB_DF_lecture_notes.pdf} Matteo Barigozzi